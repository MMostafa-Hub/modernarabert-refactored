\section{Experimental Setup}
All pretraining and fine-tuning were conducted on a high-performance node equipped with 12 CPU cores, 32~GB RAM, and a 40~GB NVIDIA A100 GPU. We evaluated \texttt{ModernAraBERT} on three Arabic NLP tasks: Sentiment Analysis (SA), Named Entity Recognition (NER), and Question Answering (QA). During benchmarking, the pretrained encoder was frozen, and only task-specific classification heads were fine-tuned to reduce training time and overfitting on limited datasets. Each task was trained for up to 200 epochs with an early stopping patience of 10, except NER, which was trained for 5 epochs.


%\paragraph{Sentiment Classification}
%We benchmarked sentiment classification using three datasets:  
%\begin{itemize}
 %   \item \textbf{Hotel Arabic Reviews Dataset (HARD)}~\cite{Elnagar2018}, comprising reviews in both Modern Standard Arabic (MSA) and dialectal Arabic. Following~\cite{antoun2020arabert}, we excluded neutral 3-star reviews, yielding a binary classification setting.  
  %  \item \textbf{Arabic Jordanian General Tweets (AJGT)} Corpus\footnote{\href{https://github.com/komari6/Arabic-twitter-corpus-AJGT}{AJGT Dataset}}, containing 1,800 tweets labeled as positive or negative.  
  %  \item \textbf{Large-Scale Arabic Book Reviews (LABR)}~\cite{aly-atiya-2013-labr}, using the unbalanced binary version for consistency with prior work.  
%\end{itemize}  

%For datasets without predefined splits, we followed a 60/20/20 train/validation/test partition. Sentence-level representations were derived from the \texttt{[CLS]} token and passed to a classification head for binary or multi-class prediction. Performance was measured using Macro F1-score. 

%\paragraph{Named Entity Recognition}
%NER experiments were performed on the ANERCorp dataset~\cite{Benajiba:2007}, using the official CAMeL Lab splits provided via HuggingFace~\cite{obeid2020camel}. The dataset includes entities such as \texttt{Person}, \texttt{Location}, and \texttt{Organization}.  

%We adopted the IOB2 tagging scheme~\cite{Ramshaw1999}. To ensure correct alignment under subword tokenization:  
%- the first subtoken of each word was assigned the gold entity label,  
%- continuation subtokens were either mapped to the corresponding I-label (e.g., \texttt{B-PER} $\rightarrow$ \texttt{I-PER}) if available, or masked with \texttt{-100} during loss computation.  

%This setup ensures entity-level consistency and avoids label fragmentation across subtokens. A token classification head was placed above the encoder, with evaluation reported as macro F1-score at the entity level, following established NER practice.  

%\paragraph{Question Answering}
%For extractive QA, we combined Arabic-SQuAD~\cite{Mozannar:2019} with 50\% of ARCD~\cite{mozannar-etal-2019-neural} as training data, reserving the remaining 50\% of ARCD for testing. This setup provides both coverage and comparability with prior Arabic QA studies.  

%The QA head comprised the pretrained encoder, a prediction layer, and a linear classifier producing start and end span logits. Regularization was applied via dropout (0.1). Hyperparameters included 200 training epochs, AdamW optimizer (learning rate $3\times10^{-5}$), batch sizes of 64 for \texttt{AraBERT} and 32 for \texttt{ModernAraBERT}, and early stopping based on validation F1.  

%Question–context pairs were tokenized to a maximum of 512 tokens with a document stride of 128 for long contexts. Character-level answer spans were mapped to token indices, and cross-entropy loss was computed jointly over start and end positions. During inference, the predicted answer span was extracted by selecting the start–end token pair with maximum joint probability.  Evaluation followed Exact Match (EM) metric, providing complementary measures of exactness.  



\paragraph{Sentiment Classification}
We evaluated sentiment classification on three benchmark datasets. The Hotel Arabic Reviews Dataset (HARD)~\cite{Elnagar2018} contains both Modern Standard Arabic (MSA) and dialectal reviews; following~\cite{antoun2020arabert}, neutral 3-star entries were excluded to form a binary classification setup. The Arabic Jordanian General Tweets (AJGT) corpus\footnote{\href{https://github.com/komari6/Arabic-twitter-corpus-AJGT}{AJGT Dataset}} consists of 1,800 tweets labeled as positive or negative, while the Large-Scale Arabic Book Reviews (LABR) dataset~\cite{aly-atiya-2013-labr} was used in its unbalanced binary version for comparability with prior work. For datasets without predefined splits, we adopted a 60/20/20 train/validation/test partition. Sentence-level representations were obtained from the \texttt{[CLS]} token and passed through a classification head, with performance evaluated using the Macro F1-score.

\paragraph{Named Entity Recognition}
NER experiments were conducted on the ANERCorp dataset~\cite{Benajiba:2007} using the official CAMeL Lab splits~\cite{obeid2020camel}, which include entities such as \texttt{Person}, \texttt{Location}, and \texttt{Organization}. The IOB2 tagging scheme~\cite{Ramshaw1999} was employed, assigning the gold entity label to the first subtoken of each word, while continuation subtokens were mapped to the corresponding I-label or masked with \texttt{-100} during loss computation. A token classification head was placed on top of the encoder, and results were reported as entity-level macro F1-scores, following standard NER evaluation practice.

\paragraph{Question Answering}
For extractive question answering, we combined Arabic-SQuAD~\cite{Mozannar:2019} with 50\% of ARCD~\cite{mozannar-etal-2019-neural} for training and reserved the remaining ARCD data for testing. The model architecture included the pretrained encoder, a prediction layer, and a linear classifier for start and end span logits. Training employed AdamW (learning rate $3\times10^{-5}$) with dropout (0.1), a maximum of 200 epochs, and early stopping based on validation F1. Inputs were tokenized to a maximum length of 512 with a document stride of 128, and character-level answer spans were aligned with token indices. Cross-entropy loss was computed jointly over start and end positions, and during inference, the span with the highest joint probability was selected. Evaluation was performed using the Exact Match (EM) metric.




\section{Results and Discussion}

%\subsection{Sentiment Classification}
%Table~\ref{tab:sentiment_results} presents the SA accuracy results for our adapted \texttt{ModernAraBERT} compared to the \texttt{AraBERT} baseline. \texttt{ModernAraBERT} outperforms the baseline AraBERT at all datasets despite only training the prediction head. This suggests that the features learned during pre-training on Arabic data were effectively transferred.
\subsection{Sentiment Classification}
Table~\ref{tab:sentiment_results} reports the Macro-F1 scores for sentiment classification across the LABR, HARD, and AJGT datasets. \texttt{ModernAraBERT} achieves the best performance on all three benchmarks, surpassing prior Arabic models by a substantial margin. Specifically, it improves over the most widely adopted baseline (\texttt{AraBERTv1}) by +11\% on LABR, +16\% on HARD, and +12\% on AJGT. Notably, while \texttt{mBERT} and \texttt{MARBERT} perform competitively on AJGT, they lag considerably on MSA-heavy datasets such as LABR and HARD. These results indicate that the pretraining of \texttt{ModernAraBERT} on large-scale Arabic corpora captures both standard and dialectal features effectively, yielding robust transfer across diverse sentiment domains despite limited fine-tuning.


\begin{table}[ht]
    \centering
    \small % or \footnotesize if needed
    \caption{Macro-F1 (\%) comparison of \texttt{ModernAraBERT} and other Arabic language models across sentiment datasets. Best scores per dataset are in bold.}
    \label{tab:sentiment_results}
    \begin{tabular}{l@{\hspace{0.4cm}}c@{\hspace{0.4cm}}c@{\hspace{0.4cm}}c}
        \toprule
        \textbf{Model} & \textbf{LABR} & \textbf{HARD} & \textbf{AJGT} \\
        \midrule
        AraBERTv1 & 45.35 & 72.65 & 58.01 \\
        AraBERTv2 & 45.79 & 67.10 & 53.59 \\
        mBERT     & 44.18 & 71.70 & 61.55 \\
        MARBERT   & 45.54 & 67.39 & 60.63 \\
        \textbf{ModernAraBERT} & \textbf{56.45} & \textbf{89.37} & \textbf{70.54} \\
        \bottomrule
    \end{tabular}\\
    \vspace{0.1cm}
\end{table}


\subsection{Named Entity Recognition (NER)}
\label{sec:ner_results}
Table~\ref{tab:ner_results} presents the Macro-F1 results for the NER task. \texttt{ModernAraBERT} achieves a clear improvement over all baselines, reaching 28.23\% compared to 16.77\% for \texttt{AraBERTv2} and 13.46\% for \texttt{AraBERTv1}. This represents a relative gain of over 68\% against the strongest prior Arabic model. The performance gap is even larger when compared to \texttt{mBERT} and \texttt{MARBERT}, which struggle to generalize beyond domain-specific training. These results highlight the ability of \texttt{ModernAraBERT} to learn richer token-level representations that effectively capture Arabic morphological and contextual cues, improving entity boundary detection despite limited fine-tuning.

\begin{table}[ht]
    \centering
    \small % or \footnotesize if needed
    \caption{Macro-F1 (\%) comparison of \texttt{ModernAraBERT} and other models on the NER task. Best score is in bold.}
    \label{tab:ner_results}
    \begin{tabular}{l@{\hspace{0.4cm}}c}
        \toprule
        \textbf{Model} & \textbf{NER (Macro-F1)} \\
        \midrule
        AraBERTv1   & 13.46 \\
        AraBERTv2   & 16.77 \\
        mBERT       & 12.15 \\
        MARBERT     & 7.42  \\
        \textbf{ModernAraBERT} & \textbf{28.23} \\
        \bottomrule
    \end{tabular}\\
    \vspace{0.1cm}
\end{table}


\subsection{Question Answering}
Table~\ref{tab:qa_results} presents the Exact Match (EM) results on the ARCD test split. \texttt{ModernAraBERT} achieves the highest EM score of 27.10\%, outperforming all baselines including \texttt{AraBERTv2} (26.08\%), \texttt{AraBERT} (25.36\%), \texttt{mBERT} (25.12\%), and \texttt{MARBERT} (23.58\%). Although the absolute gains appear moderate, they are consistent across all baselines, demonstrating the effectiveness of the adapted model in capturing precise answer spans. The improvements indicate that pretraining on large-scale Arabic corpora enhances contextual understanding and span localization, enabling \texttt{ModernAraBERT} to better align question and context representations in extractive QA tasks.

\begin{table}[ht]
    \centering
    \small
    \caption{Extractive Question Answering Results (Exact Match, \%) on \texttt{ARCD} Test Split.}
    \label{tab:qa_results}
    \begin{tabular}{l@{\hspace{0.4cm}}c}
        \toprule
        \textbf{Model} & \textbf{EM} \\
        \midrule
        AraBERT     & 25.36 \\
        AraBERTv2   & 26.08 \\
        mBERT       & 25.12 \\
        MARBERT   & 23.58 \\
        \textbf{ModernAraBERT} & \textbf{27.10} \\
        \bottomrule
    \end{tabular}\\
    \vspace{0.1cm}
    \footnotesize \textit{EM: Exact Match.}
\end{table}


\subsection{Overall Analysis}
Across all evaluated NLP tasks, adapting an English-pretrained model like \texttt{ModernAraBERT} to Arabic through our two-phase approach with complete pretraining followed by lightweight fine-tuning of task-specific heads yielded consistently strong performance. The model outperformed \texttt{AraBERTv1}, \texttt{AraBERTv2}, \texttt{MARBERT}, and \texttt{mBERT} baselines, with the most notable gains observed in NER and QA. These results demonstrate the effectiveness of leveraging large-scale English architectures for Arabic transfer learning while maintaining reasonable computational efficiency. 

%\subsection{Hardware Resource Usage}
% our comparison of computational efficiency during the NLP tasks benchmarking revealed trade-offs between the models. Table~\ref{tab:hardware_usage} summarizes memory consumption across models and tasks during head training. Overall, \texttt{AraBERT} remained the most memory-efficient model, showing the lowest RAM and VRAM usage across benchmarks. In contrast, \texttt{ModernAraBERT} required higher resources, particularly in QA (5.90~GB RAM, 20.84~GB VRAM) and SA (9.85~GB RAM, 20.63~GB VRAM), reflecting the increased parameter footprint introduced by the extended Arabic vocabulary and larger architecture. Nevertheless, this higher computational cost was accompanied by substantial performance improvements across all tasks. The observed trade-off suggests that while \texttt{AraBERT} remains advantageous for memory-constrained or latency-critical environments, \texttt{ModernAraBERT} provides a better balance of accuracy and resource utilization for high performance Arabic NLP applications.

\subsection{Hardware Resource Usage}
Beyond accuracy, Table~\ref{tab:hardware_usage} summarizes memory consumption across models during head training. \texttt{AraBERT} was the most memory-efficient, showing the lowest RAM and VRAM usage across all tasks, while \texttt{ModernAraBERT} required more resources particularly in QA (5.90~GB RAM, 20.84~GB VRAM) and SA (9.85~GB RAM, 20.63~GB VRAM) due to its larger architecture and extended vocabulary. This increase in computational cost was offset by consistent performance gains, indicating that \texttt{ModernAraBERT} offers a favorable trade-off between accuracy and resource utilization, whereas \texttt{AraBERT} remains better suited for memory-constrained or latency-sensitive scenarios.

\begin{table}[ht]
    \centering
    \small
    \caption{Hardware Resource Usage Across Models and Benchmarks (Memory usage in GB)}
    \label{tab:hardware_usage}
    \begin{tabular}{l@{\hspace{0.25cm}}l@{\hspace{0.25cm}}c@{\hspace{0.25cm}}c}
        \toprule 
        \textbf{Benchmark} & \textbf{Model} & \textbf{Peak RAM} & \textbf{Peak VRAM} \\
        \midrule 
        \multirow{3}{*}{QA} & AraBERT & 4.52 & 13.50 \\
                             & mBERT & 1.57 & 13.66 \\
                             & MARBERT & 1.93 & 13.60 \\
                             & ModernAraBERT & 5.90 & 20.84 \\
        \hline
        \multirow{3}{*}{NER} & AraBERT & 5.55 & 6.95 \\
                            & mBERT & 4.85 & 7.91 \\
                            & MARBERT & 7.76 & 7.40 \\
                            & ModernAraBERT & 6.49 & 10.42 \\
        \hline
        \multirow{3}{*}{SA} & AraBERT & 8.34 & 13.50 \\
                            & mBERT & 8.36 & 13.66 \\
                            & MARBERT & 8.28 & 13.61 \\
                            & ModernAraBERT & 9.85 & 20.63 \\
        \bottomrule
    \end{tabular}
\end{table}