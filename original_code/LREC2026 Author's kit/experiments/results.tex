
\section{Results and Discussion}

This section reports the empirical results of \texttt{ModernAraBERT} on the three mentioned downstream Arabic NLP tasks. Our discussion emphasizes both absolute performance gains and relative improvements over baselines, highlighting the effectiveness of continued pretraining as well as any trade-offs observed.

\subsection{Sentiment Analysis}
Table~\ref{tab:sentiment_results} summarizes performance on the three sentiment datasets. Across all benchmarks, \texttt{ModernAraBERT} achieves substantial improvements over both \texttt{AraBERT v1} and \texttt{mBERT}, despite only fine-tuning the prediction head. This demonstrates that continued pretraining on Arabic corpora effectively enhances the model’s sentence-level representations, yielding strong transfer to downstream classification tasks.

Performance gains are particularly notable on the HARD dataset (+16.7~\% over \texttt{AraBERT}, +17.7~\% over \texttt{mBERT}), which contains both MSA and dialectal Arabic, indicating that the model captures cross-variant sentiment signals more effectively. On AJGT and LABR, improvements of 12.5~\% and 11~\% respectively confirm robustness across both small-scale (tweets) and large-scale (book reviews) corpora. These consistent gains suggest that \texttt{ModernAraBERT} generalizes well across domains and genre variations.

\begin{table}[ht]
    \centering
    \label{tab:sentiment_results}
    \begin{tabular}{l@{\hspace{0.25cm}}c@{\hspace{0.25cm}}c@{\hspace{0.25cm}}c}
        \toprule 
        \textbf{Dataset} & \textbf{AraBERT} & \textbf{mBERT} & \textbf{ModernAraBERT} \\
        \midrule 
        AJGT  & 58.0  & 61.5 & \textbf{70.5} \\
        HARD  & 72.7 & 71.7 & \textbf{89.4} \\
        LABR  & 45.5 & 45.5 & \textbf{56.5} \\
        \bottomrule
    \end{tabular}\\
    \caption{Macro-F1 (\%) on sentiment datasets.}
    \vspace{0.1cm}
\end{table}







\subsection{Named Entity Recognition (NER)}
\label{sec:ner_results}
Table~\ref{tab:ner_results} reports the NER results on ANERCorp. \texttt{ModernAraBERT} achieved a micro F1-score of 82.1~\%, surpassing \texttt{AraBERT v1} (78.9\%) while remaining below \texttt{mBERT} (90.7\%). These results highlight two important observations. First, continued pretraining on Arabic corpora improves token-level representation quality relative to \texttt{AraBERT}, confirming the benefits of our adaptation strategy for sequence labeling. Second, the superior performance of \texttt{mBERT} indicates that multilingual training at scale may provide broader cross-lingual generalization for this task, potentially due to larger training diversity and multilingual alignment.  

%We also examined computational efficiency. \texttt{AraBERT} achieved higher throughput (925.4 samples/sec vs. 495.8 for \texttt{ModernAraBERT}) and required less VRAM (0.52~GB vs. 0.83~GB), while RAM usage was comparable (1.30~GB vs. 1.41~GB). This points to a trade-off: \texttt{ModernAraBERT} offers higher accuracy than \texttt{AraBERT}, but at the cost of slower inference and higher GPU memory consumption.  

\begin{table}[ht]
    \centering
    \label{tab:ner_results}
    \begin{tabular}{l@{\hspace{0.3cm}}c}
        \toprule \textbf{Model} & \textbf{Micro F1} \\
        \midrule 
        AraBERT~\cite{antoun2020arabert} & 78.9 \\
        mBERT                           & \textbf{90.7} \\
        ModernAraBERT                   & 82.1 \\
        \bottomrule
    \end{tabular}\\
    \caption{Named Entity Recognition results (Micro F1-score, \%) on \texttt{ANERCorp}.}
    \vspace{0.1cm}
\end{table}

\subsection{Question Answering (QA)}
Results for the QA task are presented in Table~\ref{tab:qa_results}. \texttt{ModernAraBERT} consistently outperformed both baselines across all evaluation metrics. Compared to \texttt{AraBERT}, our model achieved a 41.3\% relative gain in Exact Match (18.73 vs. 13.26), a 15.6\% relative gain in F1-score (47.18 vs. 40.82), and a 7.3\% relative gain in Sentence Match (76.66 vs. 71.47). These improvements indicate that \texttt{ModernAraBERT} not only produces more exact matches but also captures semantic information more effectively at both token and sentence levels.  

Interestingly, while \texttt{mBERT} performed competitively (15.27 EM, 46.12 F1-Score), it lagged behind in Sentence Match (63.11), suggesting weaker ability to capture longer-span semantic coherence. By contrast, \texttt{ModernAraBERT} demonstrated stronger semantic alignment, which we attribute to the continued pretraining on Arabic corpora and task-specific fine-tuning.  

\begin{table}[ht]
    \centering
    \small
    \label{tab:qa_results}
    \begin{tabular}{l@{\hspace{0.25cm}}c@{\hspace{0.25cm}}c@{\hspace{0.25cm}}c}
        \toprule \textbf{Model} & \textbf{EM} & \textbf{F1} & \textbf{SM} \\
        \midrule 
        AraBERT~\cite{antoun2020arabert} & 13.26 & 40.82 & 71.47 \\
        mBERT                           & 15.27 & 46.12 & 63.11 \\
        ModernAraBERT                   & \textbf{18.73} & \textbf{47.18} & \textbf{76.66} \\
        \bottomrule
    \end{tabular}\\
    \caption{Extractive QA results (\%) on \texttt{ARCD} Test Split.}
    \vspace{0.1cm}
    \footnotesize \textit{EM: Exact Match, F1: token-level F1, SM: Sentence Match.}
\end{table}


\subsection{Overall Analysis}
Across all evaluated tasks, adapting an English pretrained model to Arabic through our two phase strategy continued pretraining followed by task-specific fine-tuning proved highly effective. \texttt{ModernAraBERT} consistently outperformed the Arabic-specific \texttt{AraBERT v1} 
% TODO: add NER and mBERT if we outperformed it
baseline, with particularly strong gains in sentiment analysis and question answering. These improvements confirm that resource-efficient language adaptation can rival or surpass monolingual models trained from scratch.  

At the same time, results underscore important trade-offs. While \texttt{ModernAraBERT} achieved higher accuracy, \texttt{AraBERT} maintained superior inference throughput achieving a higher throughput of 925.4 samples/sec vs. 495.8 for \texttt{ModernAraBERT} and lower GPU memory usage, especially evident in NER experiments. This suggests that model selection should be guided by application requirements: latency-critical or resource-constrained scenarios may favor \texttt{AraBERT}, whereas accuracy-oriented deployments benefit more from \texttt{ModernAraBERT}. The competitive performance of \texttt{mBERT} in NER further highlights the potential of large multilingual pretraining for token-level tasks, though it lagged behind in sentence-level semantic alignment (SM) for QA.  

\subsection{Hardware Resource Usage}
Table~\ref{tab:hardware_usage} summarizes peak memory consumption across benchmarks. A consistent pattern emerges: \texttt{AraBERT} is the most memory-efficient model, while \texttt{ModernAraBERT} incurs higher VRAM usage, particularly for QA (3.22~GB vs.~2.07~GB for \texttt{AraBERT}). RAM usage remained broadly similar across models, with only minor fluctuations.  

These findings confirm that \texttt{ModernAraBERT} offers accuracy improvements at the cost of increased GPU memory demand, especially in span-extraction tasks. From a practical perspective, this trade-off is acceptable for research and enterprise environments with sufficient GPU capacity, but may limit deployment on edge devices or latency-sensitive pipelines. By contrast, \texttt{AraBERT} remains attractive for lightweight applications, while \texttt{mBERT} provides a balanced middle ground for scenarios requiring cross-lingual portability.  

\begin{table}[!ht]
\label{tab:hardware_usage}
\begin{center}
\small
\begin{tabularx}{\columnwidth}{|l|l|X|X|}
      \hline
      \textbf{Benchmark} & \textbf{Model} & \textbf{Peak RAM} & \textbf{Peak VRAM} \\
      \midrule 
      \multirow{3}{*}{NER} & AraBERT & 1.53 & 0.52 \\
                           & mBERT & 1.60 & 0.68 \\
                           & ModernAraBERT & 1.49 & 0.83 \\
      \hline
      \multirow{3}{*}{QA}  & AraBERT & 1.42 & 2.07 \\
                           & mBERT & 1.46 & 2.84 \\
                           & ModernAraBERT & 1.39 & 3.22 \\
      \hline
      \multirow{3}{*}{SA}  & AraBERT & 1.65 & 0.52 \\
                           & mBERT & 1.66 & 0.68 \\
                           & ModernAraBERT & 1.36 & 0.82 \\
      \hline
\end{tabularx}
\caption{Hardware resource usage across models and benchmarks (GB).}
\end{center}
\end{table}

Although our experiments focus on Arabic, the proposed methodology is inherently language-agnostic. By leveraging English-pretrained backbones such as \texttt{ModernBERT} and applying language-adaptive pretraining on target corpora, similar strategies can be employed for other morphologically rich or low-resource languages. This reduces the need to train monolingual models from scratch, a process that is often prohibitively expensive in both data and compute.  
The results suggest that languages with limited dedicated resources may benefit disproportionately from this approach. For instance, languages such as Amharic, Urdu, or Kazakh—which face challenges of sparse annotated data and high morphological complexity could be supported through continued pretraining on monolingual corpora while reusing the cross-lingual knowledge encoded in large English models~\cite{Wiemerslage2022MorphologicalPO}.  

From a community perspective, this paradigm promotes inclusivity by lowering the barrier to building competitive NLP models for underrepresented languages. It complements multilingual models like \texttt{mBERT} by offering a targeted, resource-efficient alternative that can achieve stronger task-specific performance without requiring massive multilingual pretraining. Thus, the broader impact of our work lies in presenting a scalable framework for extending state-of-the-art NLP to languages that remain marginalized in the current landscape of large language models.