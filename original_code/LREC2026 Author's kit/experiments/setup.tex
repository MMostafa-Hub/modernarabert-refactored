\section{Experimental Setup}

To assess the effectiveness of our proposed adaptation strategy, we evaluate \texttt{ModernAraBERT} on three representative Arabic NLP tasks: sentiment analysis (SA), named entity recognition (NER), and extractive question answering (QA). These tasks were chosen as they collectively cover sentence-level, sequence-labeling, and span-extraction settings, providing a comprehensive evaluation of model capabilities. We compare against two strong baselines: \texttt{AraBERT v1}, the most established Arabic-specific BERT variant, and \texttt{mBERT}, a widely adopted multilingual model.  

All experiments follow a controlled fine-tuning protocol where the pretrained encoder is kept frozen, and only lightweight task-specific heads are optimized. This setup isolates the contribution of continued pretraining on Arabic corpora, while reducing the risk of overfitting on limited task data. Detailed descriptions of hardware, datasets, and training configurations are provided below.

\subsection{Computational Environment}
All pretraining and fine-tuning experiments were conducted on a high-performance computing node equipped with 12 CPU cores, 32~GB RAM, and a 40~GB NVIDIA A100 GPU. These specifications constrained the choice of sequence length (512 tokens) to ensure comparability with \texttt{AraBERT} while remaining feasible within GPU memory.  

\subsection{Fine-Tuning Strategy}
During this phase, the pretrained encoder parameters were frozen and only the task-specific classification heads were fine-tuned. This strategy was selected to 
(i) reduce training time,  
(ii) minimize overfitting on relatively small task datasets, and  
(iii) assess the quality of representations obtained during continued pretraining.  

Unless otherwise specified, all tasks were trained with a maximum of 200 epochs, early stopping patience of 10 epochs, AdamW optimizer, and a dropout ratio of 0.1 for regularization. An exception was NER, which converged reliably within 5 epochs.

\subsection{Sentiment Analysis}
We benchmarked sentiment classification using three datasets:  
\begin{itemize}
    \item \textbf{Hotel Arabic Reviews Dataset (HARD)}~\cite{Elnagar2018}, comprising reviews in both Modern Standard Arabic (MSA) and dialectal Arabic. Following~\cite{antoun2020arabert}, we excluded neutral 3-star reviews, yielding a binary classification setting.  
    \item \textbf{Arabic Jordanian General Tweets (AJGT)} Corpus\footnote{\href{https://github.com/komari6/Arabic-twitter-corpus-AJGT}{AJGT Dataset}}, containing 1,800 tweets labeled as positive or negative.  
    \item \textbf{Large-Scale Arabic Book Reviews (LABR)}~\cite{aly-atiya-2013-labr}, using the unbalanced binary version for consistency with prior work.  
\end{itemize}  

For datasets without predefined splits, we followed a 60/20/20 train/validation/test partition. Sentence-level representations were derived from the \texttt{[CLS]} token and passed to a classification head for binary or multi-class prediction. Performance was measured using Macro F1-score.  

\subsection{Named Entity Recognition}
NER experiments were performed on the ANERCorp dataset~\cite{Benajiba:2007}, using the official CAMeL Lab splits provided via HuggingFace~\cite{obeid2020camel}. The dataset includes entities such as \texttt{Person}, \texttt{Location}, and \texttt{Organization}.  

We adopted the IOB2 tagging scheme~\cite{Ramshaw1999}. To ensure correct alignment under subword tokenization:  
- the first subtoken of each word was assigned the gold entity label,  
- continuation subtokens were either mapped to the corresponding I-label (e.g., \texttt{B-PER} $\rightarrow$ \texttt{I-PER}) if available, or masked with \texttt{-100} during loss computation.  

This setup ensures entity-level consistency and avoids label fragmentation across subtokens. A token classification head was placed above the encoder, with evaluation reported as micro F1-score at the entity level, following established NER practice.  

\subsection{Question Answering}
For extractive QA, we combined Arabic-SQuAD~\cite{Mozannar:2019} with 50\% of ARCD~\cite{mozannar-etal-2019-neural} as training data, reserving the remaining 50\% of ARCD for testing. This setup provides both coverage and comparability with prior Arabic QA studies.  

The QA head comprised the pretrained encoder, a prediction layer, and a linear classifier producing start and end span logits. Regularization was applied via dropout (0.1). Hyperparameters included 200 training epochs, AdamW optimizer (learning rate $3\times10^{-5}$), batch sizes of 64 for \texttt{AraBERT} and 32 for \texttt{ModernAraBERT}, and early stopping based on validation F1.  

Question–context pairs were tokenized to a maximum of 512 tokens with a document stride of 128 for long contexts. Character-level answer spans were mapped to token indices, and cross-entropy loss was computed jointly over start and end positions. During inference, the predicted answer span was extracted by selecting the start–end token pair with maximum joint probability.  

Evaluation followed standard extractive QA metrics: Exact Match (EM), token-level F1, and Sentence Match (SM), providing complementary measures of exactness, token overlap, and semantic alignment.  


In summary, our experimental setup was designed to provide a rigorous and fair evaluation of \texttt{ModernAraBERT}. We assessed the model across three complementary task types—sentence-level classification (SA), sequence labeling (NER), and span extraction (QA)—using widely adopted benchmark datasets. Comparisons against both Arabic-specific (\texttt{AraBERT v1}) and multilingual (\texttt{mBERT}) baselines ensure that our evaluation is representative of the state of the art. By freezing the encoder and fine-tuning only lightweight task-specific heads, we isolate the contribution of continued pretraining on Arabic corpora while controlling for overfitting. The following section presents the results of these experiments, highlighting both performance gains and computational trade-offs relative to the baselines.

