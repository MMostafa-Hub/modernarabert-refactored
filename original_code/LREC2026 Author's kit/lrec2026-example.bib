%%% ====================================================================
%%%  BibTeX-file{
%%%     author          = "David Rhead",
%%%     version         = "1.00",
%%%     date            = "17 Feb 1990",
%%%     time            = "17:00 GMT",
%%%     filename        = "test.bib",
%%%     address         = "Cripps Computing Centre,
%%%                        University of Nottingham,
%%%                        University Park,
%%%                        Nottingham,
%%%                        NG7 2RD,
%%%                        United Kingdom",
%%%     telephone       = "+44 602 484848 Ext 2670",
%%%     FAX             = "+44 602 588138",
%%%     checksum        = "05151 839 2908 25082",
%%%     email           = "David_Rhead at uk.ac.nott.vme (JANET)",
%%%     codetable       = "ISO/ASCII",
%%%     keywords        = "bibliography, citation, references",
%%%     supported       = "no",
%%%     docstring       = "This BibTeX database file contains entries
%%%                        designed for testing whether a BibTeX style
%%%                        file lays references out as recommended by
%%%                        certain authorities.  (Note, however, that
%%%                        the BS 1629 examples are from the 1976
%%%                        edition.  The file needs updating to use
%%%                        examples from the 1989 edition instead.)
%%%
%%%                        The checksum field above contains a CRC-16
%%%                        checksum as the first value, followed by the
%%%                        equivalent of the standard UNIX wc (word
%%%                        count) utility output of lines, words, and
%%%                        characters.  This is produced by Robert
%%%                        Solovay's checksum utility.",
%%%  }
%%% ====================================================================
%% @COMMENT{Some other standard works describing conventions for citations
%%      and bibliographies}


@article{antoun2020arabert,
  title={AraBERT: Transformer-based Model for Arabic Language Understanding},
  author={Antoun, Wissam and Baly, Fady and Hajj, Hazem},
  journal={arXiv preprint arXiv:2003.00104},
  year={2020}
}

@inproceedings{abdelali2016farasa,
  title={Farasa: A Fast and Furious Segmenter for Arabic},
  author={Abdelali, Ahmed and Darwish, Kareem and Durrani, Nadir and Mubarak, Hamdy},
  booktitle={Proceedings of NAACL Demonstrations},
  pages={11--16},
  year={2016}
}

@article{wolf2020transformers,
  title={Transformers: State-of-the-Art Natural Language Processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and others},
  journal={Proceedings of EMNLP 2020: System Demonstrations},
  pages={38--45},
  year={2020}
}

@article{abdul2021marbert,
  title={MARBERT: Arabic Language Model in the Wild},
  author={Abdul-Mageed, Muhammad and Elmadany, AbdelRahim and Nagoudi, El Moatez Billah},
  journal={arXiv preprint arXiv:2101.05785},
  year={2021}
}

@inproceedings{abdelali2021qari,
  title={QARiB: QCRI Arabic and Dialectal BERT},
  author={Abdelali, Ahmed and Darwish, Kareem and Mubarak, Hamdy and Tomeh, Nadi},
  booktitle={Proceedings of the Sixth Arabic Natural Language Processing Workshop},
  pages={52--59},
  year={2021}
}

@inproceedings{inoue2021camelbert,
  title={CAMeL BERT: Pre-trained Language Models for Arabic},
  author={Inoue, Go and Khalifa, Salam and Zaghouani, Wajdi and Habash, Nizar},
  booktitle={Proceedings of the Sixth Arabic NLP Workshop},
  pages={32--41},
  year={2021}
}

@inproceedings{pasha2014madamira,
  title={MADAMIRA: A Fast, Comprehensive Tool for Morphological Analysis and Disambiguation of Arabic},
  author={Pasha, Arfath and others},
  booktitle={Proceedings of LREC},
  pages={1094--1101},
  year={2014}
}

@inproceedings{10.1007/978-3-540-70939-8_13,
  author    = {Benajiba, Yassine
               and Rosso, Paolo
               and Bened{\'i}Ruiz, Jos{\'e} Miguel},
  editor    = {Gelbukh, Alexander},
  title     = {ANERsys: An Arabic Named Entity Recognition System Based on Maximum Entropy},
  booktitle = {Computational Linguistics and Intelligent Text Processing},
  year      = {2007},
  publisher = {Springer Berlin Heidelberg},
  address   = {Berlin, Heidelberg},
  pages     = {143--153},
  abstract  = {The task of Named Entity Recognition (NER) allows to identify proper names as well as temporal and numeric expressions, in an open-domain text. NER systems proved to be very important for many tasks in Natural Language Processing (NLP) such as Information Retrieval and Question Answering tasks. Unfortunately, the main efforts to build reliable NER systems for the Arabic language have been made in a commercial frame and the approach used as well as the accuracy of the performance are not known. In this paper, we present ANERsys: a NER system built exclusively for Arabic texts based-on n-grams and maximum entropy. Furthermore, we present both the specific Arabic language dependent heuristic and the gazetteers we used to boost our system. We developed our own training and test corpora (ANERcorp) and gazetteers (ANERgazet) to train, evaluate and boost the implemented technique. A major effort was conducted to make sure all the experiments are carried out in the same framework of the CONLL 2002 conference. We carried out several experiments and the preliminary results showed that this approach allows to tackle successfully the problem of NER for the Arabic language.},
  isbn      = {978-3-540-70939-8}
}

@inproceedings{Benajiba:2007,
   AUTHOR     = {Benajiba, Yassine
               AND Rosso, Paolo
               AND Bened{\'i}ruiz, Jos{\'e} Miguel},
   EDITOR     = {Gelbukh, Alexander},
   TITLE      = {ANERsys: An Arabic Named Entity Recognition System Based on Maximum Entropy},
   BOOKTITLE  = {Computational Linguistics and Intelligent Text Processing},
   YEAR       = {2007},
   PUBLISHER  = {Springer Berlin Heidelberg},
   ADDRESS    = {Berlin, Heidelberg},
   PAGES      = {143--153},
   ABSTRACT   = {The task of Named Entity Recognition (NER) allows to identify proper names as well as temporal and numeric expressions, in an open-domain text. NER systems proved to be very important for many tasks in Natural Language Processing (NLP) such as Information Retrieval and Question Answering tasks. Unfortunately, the main efforts to build reliable NER systems for the Arabic language have been made in a commercial frame and the approach used as well as the accuracy of the performance are not known. In this paper, we present ANERsys: a NER system built exclusively for Arabic texts based-on n-grams and maximum entropy. Furthermore, we present both the specific Arabic language dependent heuristic and the gazetteers we used to boost our system. We developed our own training and test corpora (ANERcorp) and gazetteers (ANERgazet) to train, evaluate and boost the implemented technique. A major effort was conducted to make sure all the experiments are carried out in the same framework of the CONLL 2002 conference. We carried out several experiments and the preliminary results showed that this approach allows to tackle successfully the problem of NER for the Arabic language.},
   ISBN       = {978-3-540-70939-8}
}

@article{Mozannar:2019,
   TITLE      = {Neural Arabic Question Answering},
   AUTHOR     = {Mozannar, Hussein AND Others},
   JOURNAL    = {arXiv preprint arXiv:1906.05685},
   YEAR       = {2019}
}

@inproceedings{Papineni:2002,
   TITLE      = {BLEU: a method for automatic evaluation of machine translation},
   AUTHOR     = {Papineni, Kishore
               AND Roukos, Salim
               AND Ward, Todd
               AND Zhu, Wei-jing},
   BOOKTITLE  = {Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
   PAGES      = {311--318},
   YEAR       = {2002}
}

@inproceedings{Lin:2004,
   TITLE      = {ROUGE: A package for automatic evaluation of summaries},
   AUTHOR     = {Lin, Chin-Yew},
   BOOKTITLE  = {Text summarization branches out},
   PAGES      = {74--81},
   YEAR       = {2004},
   ORGANIZATION = {Association for Computational Linguistics}
}

@article{warner2024modernbert,
  title     = {Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference},
  author    = {Warner, Benjamin and Chaffin, Antoine and Clavié, Benjamin and Weller, Orion and Hallström, Oskar and Taghadouini, Said and Gallagher, Alexis and Biswas, Raja and Ladhak, Faisal and Aarsen, Tom and Cooper, Nathan and Adams, Griffin and Howard, Jeremy and Poli, Iacopo},
  journal   = {arXiv preprint arXiv:2412.13663},
  year      = {2024},
  url       = {https://doi.org/10.48550/arXiv.2412.13663}
}
@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423/",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
}

@article{Acheampong2021,
  author    = {Francisca Adoma Acheampong and Henry Nunoo-Mensah and Wenyu Chen},
  title     = {Transformer models for text-based emotion detection: a review of BERT-based approaches},
  journal   = {Artificial Intelligence Review},
  volume    = {54},
  number    = {8},
  pages     = {5789--5829},
  year      = {2021},
  doi       = {10.1007/s10462-021-09958-2},
  url       = {https://doi.org/10.1007/s10462-021-09958-2},
  issn      = {1573-7462}
}
@article{Gardazi2025,
  author    = {Nadia Mushtaq Gardazi and Ali Daud and Muhammad Kamran Malik and Amal Bukhari and Tariq Alsahfi and Bader Alshemaimri},
  title     = {BERT applications in natural language processing: a review},
  journal   = {Artificial Intelligence Review},
  volume    = {58},
  number    = {6},
  pages     = {166},
  year      = {2025},
  doi       = {10.1007/s10462-025-11162-5},
  url       = {https://doi.org/10.1007/s10462-025-11162-5},
  issn      = {1573-7462}
}

@article{MATRANE2023101570,
title = {A systematic literature review of Arabic dialect sentiment analysis},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {35},
number = {6},
pages = {101570},
year = {2023},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2023.101570},
url = {https://www.sciencedirect.com/science/article/pii/S1319157823001246},
author = {Yassir Matrane and Faouzia Benabbou and Nawal Sael},
keywords = {Dialectical Arabic, Sentiment analysis, Systematic literature review, Machine learning, Preprocessing},
abstract = {Sentiment analysis is the process of using natural language processing, computational linguistics, and other text analysis techniques to identify and extract subjective information in order to generate a judgment about the attitude or emotional state behind the text. It has been applied to many fields, including marketing, politics, and psychology. This paper presents a systematic literature review (SLR) of sentiment analysis for dialectical Arabic (DA). The variation among these dialects is primarily based on differences in grammar, vocabulary, and syntax, which makes it hard for researchers to perform polarity classification for DA. This is where our SLR comes in, assessing multiple aspects of sentiment analysis for DA as well as smoothing the advancement of researchers' works for related studies. We have identified all the steps that have a crucial influence on the machine learning model applied for dialect sentiment analysis, including text annotation, text preprocessing, feature extraction, and the approaches adopted. We have also determined the challenges and open issues of sentiment analysis for Arabic dialect (SAAD), where research efforts should be focused.}
}

@inproceedings{abdul-mageed-etal-2021-arbert,
    title = "{ARBERT} {\&} {MARBERT}: Deep Bidirectional Transformers for {A}rabic",
    author = "Abdul-Mageed, Muhammad  and
      Elmadany, AbdelRahim  and
      Nagoudi, El Moatez Billah",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.551",
    doi = "10.18653/v1/2021.acl-long.551",
    pages = "7088--7105",
    abstract = "Pre-trained language models (LMs) are currently integral to many natural language processing systems. Although multilingual LMs were also introduced to serve many languages, these have limitations such as being costly at inference time and the size and diversity of non-English data involved in their pre-training. We remedy these issues for a collection of diverse Arabic varieties by introducing two powerful deep bidirectional transformer-based models, ARBERT and MARBERT. To evaluate our models, we also introduce ARLUE, a new benchmark for multi-dialectal Arabic language understanding evaluation. ARLUE is built using 42 datasets targeting six different task clusters, allowing us to offer a series of standardized experiments under rich conditions. When fine-tuned on ARLUE, our models collectively achieve new state-of-the-art results across the majority of tasks (37 out of 48 classification tasks, on the 42 datasets). Our best model acquires the highest ARLUE score (77.40) across all six task clusters, outperforming all other models including XLM-R Large ( 3.4x larger size). Our models are publicly available at https://github.com/UBC-NLP/marbert and ARLUE will be released through the same repository.",
}

@inproceedings{inoue-etal-2021-interplay,
    title = "The Interplay of Variant, Size, and Task Type in {A}rabic Pre-trained Language Models",
    author = "Inoue, Go  and
      Alhafni, Bashar  and
      Baimukan, Nurpeiis  and
      Bouamor, Houda  and
      Habash, Nizar",
    booktitle = "Proceedings of the Sixth Arabic Natural Language Processing Workshop",
    month = apr,
    year = "2021",
    address = "Kyiv, Ukraine (Online)",
    publisher = "Association for Computational Linguistics",
    abstract = "In this paper, we explore the effects of language variants, data sizes, and fine-tuning task types in Arabic pre-trained language models. To do so, we build three pre-trained language models across three variants of Arabic: Modern Standard Arabic (MSA), dialectal Arabic, and classical Arabic, in addition to a fourth language model which is pre-trained on a mix of the three. We also examine the importance of pre-training data size by building additional models that are pre-trained on a scaled-down set of the MSA variant. We compare our different models to each other, as well as to eight publicly available models by fine-tuning them on five NLP tasks spanning 12 datasets. Our results suggest that the variant proximity of pre-training data to fine-tuning data is more important than the pre-training data size. We exploit this insight in defining an optimized system selection model for the studied tasks.",
}

@misc{abdelali2021pretraining,
    title={Pre-Training BERT on Arabic Tweets: Practical Considerations},
    author={Ahmed Abdelali and Sabit Hassan and Hamdy Mubarak and Kareem Darwish and Younes Samih},
    year={2021},
    eprint={2102.10684},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{modernbert,
      title={Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference}, 
      author={Benjamin Warner and Antoine Chaffin and Benjamin Clavié and Orion Weller and Oskar Hallström and Said Taghadouini and Alexis Gallagher and Raja Biswas and Faisal Ladhak and Tom Aarsen and Nathan Cooper and Griffin Adams and Jeremy Howard and Iacopo Poli},
      year={2024},
      eprint={2412.13663},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.13663}, 
}

@inproceedings{zeroual-etal-2019-osian,
    title = "{OSIAN}: Open Source International {A}rabic News Corpus - Preparation and Integration into the {CLARIN}-infrastructure",
    author = "Zeroual, Imad  and
      Goldhahn, Dirk  and
      Eckart, Thomas  and
      Lakhouaja, Abdelhak",
    editor = "El-Hajj, Wassim  and
      Belguith, Lamia Hadrich  and
      Bougares, Fethi  and
      Magdy, Walid  and
      Zitouni, Imed  and
      Tomeh, Nadi  and
      El-Haj, Mahmoud  and
      Zaghouani, Wajdi",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-4619/",
    doi = "10.18653/v1/W19-4619",
    pages = "175--182",
    abstract = "The World Wide Web has become a fundamental resource for building large text corpora. Broadcasting platforms such as news websites are rich sources of data regarding diverse topics and form a valuable foundation for research. The Arabic language is extensively utilized on the Web. Still, Arabic is relatively an under-resourced language in terms of availability of freely annotated corpora. This paper presents the first version of the Open Source International Arabic News (OSIAN) corpus. The corpus data was collected from international Arabic news websites, all being freely available on the Web. The corpus consists of about 3.5 million articles comprising more than 37 million sentences and roughly 1 billion tokens. It is encoded in XML; each article is annotated with metadata information. Moreover, each word is annotated with lemma and part-of-speech. the described corpus is processed, archived and published into the CLARIN infrastructure. This publication includes descriptive metadata via OAI-PMH, direct access to the plain text material (available under Creative Commons Attribution-Non-Commercial 4.0 International License - CC BY-NC 4.0), and integration into the WebLicht annotation platform and CLARIN{'}s Federated Content Search FCS."
}
@article{el20161,
  title={1.5 billion words arabic corpus},
  author={El-Khair, Ibrahim Abu},
  journal={arXiv preprint arXiv:1611.04033},
  year={2016}
}
@article{2022arXiv220106642A,
       author = {{Abadji}, Julien and {Ortiz Suarez}, Pedro and {Romary}, Laurent and {Sagot}, Beno{\^\i}t},
        title = "{Towards a Cleaner Document-Oriented Multilingual Crawled Corpus}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = 2022,
        month = jan,
          eid = {arXiv:2201.06642},
        pages = {arXiv:2201.06642},
archivePrefix = {arXiv},
       eprint = {2201.06642},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2022arXiv220106642A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@Inbook{Elnagar2018,
author="Elnagar, Ashraf
and Khalifa, Yasmin S.
and Einea, Anas",
editor="Shaalan, Khaled
and Hassanien, Aboul Ella
and Tolba, Fahmy",
title="Hotel Arabic-Reviews Dataset Construction for Sentiment Analysis Applications",
bookTitle="Intelligent Natural Language Processing: Trends and Applications",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="35--52",
abstract="Arabic language suffers from the lack of available large datasets for machine learning and sentiment analysis applications. This work adds to the recently reported large dataset BRAD, which is the largest Book Reviews in Arabic Dataset. In this paper, we introduce HARD (Hotel Arabic-Reviews Dataset), the largest Book Reviews in Arabic Dataset for subjective sentiment analysis and machine language applications. HARD comprises of 490587 hotel reviews collected from the Booking.com website. Each record contains the review text in the Arabic language, the reviewer's rating on a scale of 1 to 10 stars, and other attributes about the hotel/reviewer. We make available the full unbalanced dataset as well as a balanced subset. To examine the datasets, we implement six popular classifiers using Modern Standard Arabic (MSA) as well as Dialectal Arabic (DA). We test the sentiment analyzers for polarity and rating classifications. Furthermore, we implement a polarity lexicon-based sentiment analyzer. The findings confirm the effectiveness of the classifiers and the datasets. Our core contribution is to make this benchmark-dataset available and accessible to the research community on Arabic language.",
isbn="978-3-319-67056-0",
doi="10.1007/978-3-319-67056-0_3",
url="https://doi.org/10.1007/978-3-319-67056-0_3"
}

@inproceedings{aly-atiya-2013-labr,
    title = "{LABR}: A Large Scale {A}rabic Book Reviews Dataset",
    author = "Aly, Mohamed  and
      Atiya, Amir",
    editor = "Schuetze, Hinrich  and
      Fung, Pascale  and
      Poesio, Massimo",
    booktitle = "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = aug,
    year = "2013",
    address = "Sofia, Bulgaria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P13-2088/",
    pages = "494--498"
}

@inproceedings{mozannar-etal-2019-neural,
    title = "Neural {A}rabic Question Answering",
    author = "Mozannar, Hussein  and
      Maamary, Elie  and
      El Hajal, Karl  and
      Hajj, Hazem",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4612",
    doi = "10.18653/v1/W19-4612",
    pages = "108--118",
    abstract = "This paper tackles the problem of open domain factual Arabic question answering (QA) using Wikipedia as our knowledge source. This constrains the answer of any question to be a span of text in Wikipedia. Open domain QA for Arabic entails three challenges: annotated QA datasets in Arabic, large scale efficient information retrieval and machine reading comprehension. To deal with the lack of Arabic QA datasets we present the Arabic Reading Comprehension Dataset (ARCD) composed of 1,395 questions posed by crowdworkers on Wikipedia articles, and a machine translation of the Stanford Question Answering Dataset (Arabic-SQuAD). Our system for open domain question answering in Arabic (SOQAL) is based on two components: (1) a document retriever using a hierarchical TF-IDF approach and (2) a neural reading comprehension model using the pre-trained bi-directional transformer BERT. Our experiments on ARCD indicate the effectiveness of our approach with our BERT-based reader achieving a 61.3 F1 score, and our open domain system SOQAL achieving a 27.6 F1 score.",
}

@inproceedings{ramshaw1995,
  author    = {Lance A. Ramshaw and Mitchell P. Marcus},
  title     = {Text Chunking using Transformation-Based Learning},
  booktitle = {Third Workshop on Very Large Corpora},
  year      = {1995},
  pages     = {82--94}
}

@incollection{Ramshaw1999,
  author    = {L. A. Ramshaw and M. P. Marcus},
  editor    = {Susan Armstrong and Kenneth Church and Pierre Isabelle and Sandra Manzi and Evelyne Tzoukermann and David Yarowsky},
  title     = {Text Chunking Using Transformation-Based Learning},
  booktitle = {Natural Language Processing Using Very Large Corpora},
  year      = {1999},
  publisher = {Springer Netherlands},
  address   = {Dordrecht},
  pages     = {157--176},
  abstract  = {Transformation-based learning, a technique introduced by Eric Brill (1993b), has been shown to do part-of-speech tagging with fairly high accuracy. This same method can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive ``baseNP'' chunks. For this purpose, it is convenient to view chunking as a tagging problem by encoding the chunk structure in new tags attached to each word. In automatic tests using Treebank-derived data, this technique achieved recall and precision rates of roughly 93{\%} for baseNP chunks (trained on 950K words) and 88{\%} for somewhat more complex chunks that partition the sentence (trained on 200K words). Working in this new application and with larger template and training sets has also required some interesting adaptations to the transformation-based learning approach.},
  isbn      = {978-94-017-2390-9},
  doi       = {10.1007/978-94-017-2390-9_10},
  url       = {https://doi.org/10.1007/978-94-017-2390-9_10}
}

@article{shazeer2020glu,
  title={Glu variants improve transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
}

@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding/arXiv preprint},
  author={Devlin, J},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{alammary2022bert,
  title={BERT models for Arabic text classification: a systematic review},
  author={Alammary, Ali Saleh},
  journal={Applied Sciences},
  volume={12},
  number={11},
  pages={5720},
  year={2022},
  publisher={MDPI}
}

@inproceedings{farha2021benchmarking,
  title={Benchmarking transformer-based language models for Arabic sentiment and sarcasm detection},
  author={Farha, Ibrahim Abu and Magdy, Walid},
  booktitle={The Sixth Arabic Natural Language Processing Workshop},
  pages={21--31},
  year={2021},
  organization={Association for Computational Linguistics (ACL)}
}

@inproceedings{gururangan-etal-2020-dont,
    title = "Don{'}t Stop Pretraining: Adapt Language Models to Domains and Tasks",
    author = "Gururangan, Suchin  and
      Marasovi{\'c}, Ana  and
      Swayamdipta, Swabha  and
      Lo, Kyle  and
      Beltagy, Iz  and
      Downey, Doug  and
      Smith, Noah A.",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.740/",
    doi = "10.18653/v1/2020.acl-main.740",
    pages = "8342--8360",
    abstract = "Language models pretrained on text from a wide variety of sources form the foundation of today{'}s NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task{'}s unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance."
}

@inproceedings{pfeiffer-etal-2021-adapterfusion,
    title = "{A}dapter{F}usion: Non-Destructive Task Composition for Transfer Learning",
    author = {Pfeiffer, Jonas  and
      Kamath, Aishwarya  and
      R{\"u}ckl{\'e}, Andreas  and
      Cho, Kyunghyun  and
      Gurevych, Iryna},
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.39/",
    doi = "10.18653/v1/2021.eacl-main.39",
    pages = "487--503",
    abstract = "Sequential fine-tuning and multi-task learning are methods aiming to incorporate knowledge from multiple tasks; however, they suffer from catastrophic forgetting and difficulties in dataset balancing. To address these shortcomings, we propose AdapterFusion, a new two stage learning algorithm that leverages knowledge from multiple tasks. First, in the knowledge extraction stage we learn task specific parameters called adapters, that encapsulate the task-specific information. We then combine the adapters in a separate knowledge composition step. We show that by separating the two stages, i.e., knowledge extraction and knowledge composition, the classifier can effectively exploit the representations learned from multiple tasks in a non-destructive manner. We empirically evaluate AdapterFusion on 16 diverse NLU tasks, and find that it effectively combines various types of knowledge at different layers of the model. We show that our approach outperforms traditional strategies such as full fine-tuning as well as multi-task learning. Our code and adapters are available at AdapterHub.ml."
}

@inproceedings{obeid2020camel,
  title={CAMeL tools: An open source python toolkit for Arabic natural language processing},
  author={Obeid, Ossama and Zalmout, Nasser and Khalifa, Salam and Taji, Dima and Oudah, Mai and Alhafni, Bashar and Inoue, Go and Eryani, Fadhl and Erdmann, Alexander and Habash, Nizar},
  booktitle={Proceedings of the twelfth language resources and evaluation conference},
  pages={7022--7032},
  year={2020}
}

@inproceedings{Wiemerslage2022MorphologicalPO,
  title={Morphological Processing of Low-Resource Languages: Where We Are and What’s Next},
  author={Adam Wiemerslage and Miikka Silfverberg and Changbing Yang and Arya D. McCarthy and Garrett Nicolai and Eliana Colunga and Katharina Kann},
  booktitle={Findings},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:247518745}
}