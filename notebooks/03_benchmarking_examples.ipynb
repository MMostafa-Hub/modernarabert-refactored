{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ModernAraBERT Benchmarking Examples\n",
        "\n",
        "**Evaluate ModernAraBERT on Sentiment Analysis and Named Entity Recognition tasks**\n",
        "\n",
        "This notebook demonstrates:\n",
        "1. Running sentiment analysis benchmarks (HARD, AJGT, LABR, ASTD)\n",
        "2. Running NER benchmarks (ANERCorp)\n",
        "3. Visualizing and comparing results\n",
        "4. Reproducing paper results\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Add repository root to path\n",
        "REPO_ROOT = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
        "sys.path.insert(0, str(REPO_ROOT))\n",
        "\n",
        "print(f\"Repository root: {REPO_ROOT}\")\n",
        "print(\"‚úÖ Environment ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üòä Sentiment Analysis Benchmarks\n",
        "\n",
        "ModernAraBERT was evaluated on 4 Arabic sentiment datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üòä Sentiment Analysis Datasets\\\\n\")\n",
        "print(\"1. HARD - Hotel Arabic Reviews Dataset\")\n",
        "print(\"   - 93,700 hotel reviews\")\n",
        "print(\"   - Classes: Positive, Negative\")\n",
        "print(\"   - Split: 60/20/20\\\\n\")\n",
        "\n",
        "print(\"2. AJGT - Arabic Jordanian General Tweets\")\n",
        "print(\"   - 1,800 tweets\")\n",
        "print(\"   - Classes: Positive, Negative\")\n",
        "print(\"   - Split: 60/20/20\\\\n\")\n",
        "\n",
        "print(\"3. LABR - Large-Scale Arabic Book Reviews\")\n",
        "print(\"   - 63,000 book reviews\")\n",
        "print(\"   - Classes: 1-5 stars\")\n",
        "print(\"   - Split: Predefined train/test\\\\n\")\n",
        "\n",
        "print(\"4. ASTD - Arabic Sentiment Tweets Dataset\")\n",
        "print(\"   - 10,000 tweets\")\n",
        "print(\"   - Classes: Positive, Negative, Neutral, Mixed\")\n",
        "print(\"   - Split: Predefined train/test\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Running SA Benchmarks\n",
        "\n",
        "Use the provided script to run all SA benchmarks:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```bash\n",
        "# Run all SA benchmarks\n",
        "./scripts/benchmarking/run_sa_benchmark.sh gizadatateam/ModernAraBERT all ./results/sa\n",
        "\n",
        "# Or run specific dataset\n",
        "./scripts/benchmarking/run_sa_benchmark.sh gizadatateam/ModernAraBERT HARD ./results/sa_hard\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SA Benchmark Results (Paper)\n",
        "\n",
        "Macro-F1 scores from the paper:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sentiment Analysis results from paper\n",
        "sa_results = {\n",
        "    'Dataset': ['AJGT', 'HARD', 'LABR'],\n",
        "    'AraBERT v1': [58.0, 72.7, 45.5],\n",
        "    'mBERT': [61.5, 71.7, 45.5],\n",
        "    'ModernAraBERT': [70.5, 89.4, 56.5]\n",
        "}\n",
        "\n",
        "df_sa = pd.DataFrame(sa_results)\n",
        "print(\"üìä Sentiment Analysis Results (Macro-F1 %)\\\\n\")\n",
        "print(df_sa.to_string(index=False))\n",
        "print(\"\\\\n‚úÖ Improvements over AraBERT v1:\")\n",
        "print(f\"  AJGT: +{70.5 - 58.0:.1f}% (+{((70.5 - 58.0)/58.0)*100:.1f}% relative)\")\n",
        "print(f\"  HARD: +{89.4 - 72.7:.1f}% (+{((89.4 - 72.7)/72.7)*100:.1f}% relative)\")\n",
        "print(f\"  LABR: +{56.5 - 45.5:.1f}% (+{((56.5 - 45.5)/45.5)*100:.1f}% relative)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize SA Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot SA results\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "x = range(len(df_sa['Dataset']))\n",
        "width = 0.25\n",
        "\n",
        "bars1 = ax.bar([i - width for i in x], df_sa['AraBERT v1'], width, label='AraBERT v1', alpha=0.8)\n",
        "bars2 = ax.bar([i for i in x], df_sa['mBERT'], width, label='mBERT', alpha=0.8)\n",
        "bars3 = ax.bar([i + width for i in x], df_sa['ModernAraBERT'], width, label='ModernAraBERT', alpha=0.8)\n",
        "\n",
        "ax.set_ylabel('Macro-F1 Score (%)', fontsize=12)\n",
        "ax.set_title('Sentiment Analysis Benchmark Results', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(df_sa['Dataset'])\n",
        "ax.legend()\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ ModernAraBERT achieves the best performance across all SA datasets!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üè∑Ô∏è Named Entity Recognition Benchmark\n",
        "\n",
        "Evaluation on ANERCorp dataset with IOB2 tagging scheme.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üè∑Ô∏è ANERCorp Dataset\\\\n\")\n",
        "print(\"Description:\")\n",
        "print(\"  - 150,000 tokens manually annotated\")\n",
        "print(\"  - Entity types: PERSON, LOCATION, ORGANIZATION, MISC\")\n",
        "print(\"  - Tagging scheme: IOB2\")\n",
        "print(\"  - Labeling strategy: First-subtoken only\")\n",
        "print(\"\\\\nTraining Configuration:\")\n",
        "print(\"  - Epochs: 10\")\n",
        "print(\"  - Batch size: 8\")\n",
        "print(\"  - Learning rate: 2e-5\")\n",
        "print(\"  - Loss function: Focal Loss (Œ±=0.25, Œ≥=3.0)\")\n",
        "print(\"  - Class weights: Computed for imbalanced classes\")\n",
        "print(\"  - Early stopping: Patience=8\")\n",
        "print(\"\\\\nRun benchmark:\")\n",
        "print(\"  ./scripts/benchmarking/run_ner_benchmark.sh gizadatateam/ModernAraBERT ./results/ner\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NER Benchmark Results (Paper)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# NER results from paper\n",
        "ner_results = {\n",
        "    'Model': ['AraBERT v1', 'mBERT', 'ModernAraBERT'],\n",
        "    'Micro-F1 (%)': [78.9, 90.7, 82.1]\n",
        "}\n",
        "\n",
        "df_ner = pd.DataFrame(ner_results)\n",
        "print(\"üìä NER Results on ANERCorp (Micro-F1 %)\\\\n\")\n",
        "print(df_ner.to_string(index=False))\n",
        "print(\"\\\\nüìù Note: mBERT achieves highest score due to its multilingual pretraining.\")\n",
        "print(\"ModernAraBERT offers a good trade-off between performance and efficiency.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üìä ModernAraBERT - Complete Benchmark Results\\\\n\")\n",
        "print(\"=\"*60)\n",
        "print(\"SENTIMENT ANALYSIS (Macro-F1 %)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Dataset':<15} {'AraBERT v1':<12} {'mBERT':<12} {'ModernAraBERT':<15}\")\n",
        "print(\"-\"*60)\n",
        "print(f\"{'AJGT':<15} {58.0:<12.1f} {61.5:<12.1f} {70.5:<15.1f} ‚≠ê\")\n",
        "print(f\"{'HARD':<15} {72.7:<12.1f} {71.7:<12.1f} {89.4:<15.1f} ‚≠ê\")\n",
        "print(f\"{'LABR':<15} {45.5:<12.1f} {45.5:<12.1f} {56.5:<15.1f} ‚≠ê\")\n",
        "print()\n",
        "print(\"=\"*60)\n",
        "print(\"NAMED ENTITY RECOGNITION (Micro-F1 %)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'ANERCorp':<15} {78.9:<12.1f} {90.7:<12.1f} {82.1:<15.1f}\")\n",
        "print()\n",
        "print(\"=\"*60)\n",
        "print(\"QUESTION ANSWERING (ARCD Test)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Metric':<15} {'AraBERT v1':<12} {'mBERT':<12} {'ModernAraBERT':<15}\")\n",
        "print(\"-\"*60)\n",
        "print(f\"{'Exact Match':<15} {13.26:<12.2f} {15.27:<12.2f} {18.73:<15.2f} ‚≠ê\")\n",
        "print(f\"{'F1 Score':<15} {40.82:<12.2f} {46.12:<12.2f} {47.18:<15.2f} ‚≠ê\")\n",
        "print(f\"{'Sentence Match':<15} {71.47:<12.2f} {63.11:<12.2f} {76.66:<15.2f} ‚≠ê\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\\\n‚≠ê = Best performance\")\n",
        "print(\"\\\\n‚úÖ ModernAraBERT achieves best results on SA and QA tasks!\")\n",
        "print(\"‚úÖ Competitive NER performance with efficient architecture!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Model Comparison\n",
        "\n",
        "Compare multiple models on the same dataset:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```bash\n",
        "# Compare multiple models on SA\n",
        "for model in gizadatateam/ModernAraBERT aubmindlab/bert-base-arabertv2 bert-base-multilingual-cased; do\n",
        "    model_name=$(basename $model)\n",
        "    ./scripts/benchmarking/run_sa_benchmark.sh \"$model\" all \"./results/sa_$model_name\"\n",
        "done\n",
        "\n",
        "# Compare multiple models on NER\n",
        "for model in gizadatateam/ModernAraBERT aubmindlab/bert-base-arabertv2 bert-base-multilingual-cased; do\n",
        "    model_name=$(basename $model)\n",
        "    ./scripts/benchmarking/run_ner_benchmark.sh \"$model\" \"./results/ner_$model_name\"\n",
        "done\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìñ Additional Resources\n",
        "\n",
        "- **Detailed Benchmarking Guide**: [docs/BENCHMARKING.md](../docs/BENCHMARKING.md)\n",
        "- **Dataset Documentation**: [docs/DATASETS.md](../docs/DATASETS.md)\n",
        "- **Paper Results**: [results/README.md](../results/README.md)\n",
        "- **Configuration Files**: [configs/](../configs/)\n",
        "\n",
        "---\n",
        "\n",
        "**For production benchmarking, always use the scripts in `scripts/benchmarking/` for consistent and reproducible results!**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
