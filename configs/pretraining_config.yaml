# ModernAraBERT Pretraining Configuration
# Based on the paper: "Efficient Adaptation of English Language Models for Arabic"

# Data Configuration
data:
  links_file: "./data/links.json"
  raw_data_dir: "./data/raw"
  processed_data_dir: "./data/processed"
  train_dir: "./data/processed/train"
  val_dir: "./data/processed/validation"
  test_dir: "./data/processed/test"
  
  # Data splits
  train_split: 0.9
  val_split: 0.05
  test_split: 0.05
  
  # Preprocessing options
  preprocessing:
    remove_diacritics: true
    remove_elongation: true  # Remove tatweel
    remove_punctuation: false
    use_farasa_segmentation: true
    min_sentence_length: 5  # Minimum words per sentence
    max_sentence_length: 512  # Maximum words per sentence

# Model Configuration
model:
  base_model: "answerdotai/ModernBERT-base"
  architecture:
    num_layers: 22
    hidden_size: 768
    num_attention_heads: 12
    intermediate_size: 3072
    max_position_embeddings: 8192
    vocab_size: 50368  # Original ModernBERT
    type_vocab_size: 2
  
  # Extended vocabulary
  extended_vocab_size: 80000  # Additional Arabic tokens
  total_vocab_size: 130368  # Original + Extended

# Tokenizer Configuration
tokenizer:
  tokenizer_path: "./tokenizer"
  input_corpus_dir: "./data/processed"
  vocab_extension_size: 80000
  min_frequency: 5
  special_tokens:
    - "[PAD]"
    - "[UNK]"
    - "[CLS]"
    - "[SEP]"
    - "[MASK]"

# Training Configuration
training:
  # Hardware
  device: "cuda"  # cuda or cpu
  mixed_precision: "fp16"  # fp16, bf16, or no
  gradient_checkpointing: false
  
  # Optimization
  optimizer: "adamw"
  learning_rate: 1.0e-4
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1.0e-8
  max_grad_norm: 1.0
  
  # Learning rate schedule
  lr_scheduler: "cosine"
  warmup_steps: 10000
  num_training_steps: null  # Auto-calculated based on epochs
  
  # Training parameters
  num_epochs: 3
  batch_size: 32  # Per GPU batch size
  gradient_accumulation_steps: 1
  
  # Sequence length strategy
  # Epoch 1-2: 128 tokens (efficiency)
  # Epoch 3: 512 tokens (longer context)
  sequence_length_schedule:
    - epochs: [1, 2]
      max_length: 128
    - epochs: [3]
      max_length: 512
  
  # MLM objective
  mlm_probability: 0.15
  
  # Logging and checkpointing
  logging_steps: 100
  eval_steps: 5000
  save_steps: 5000
  save_total_limit: 5
  
  # Checkpoints
  output_dir: "./output"
  checkpoint_dir: "./checkpoints"
  resume_from_checkpoint: null  # Path to checkpoint or null
  
  # Seeds for reproducibility
  seed: 42
  data_seed: 42

# Validation Configuration
validation:
  eval_strategy: "steps"
  eval_steps: 5000
  eval_batch_size: 64
  metrics:
    - "loss"
    - "perplexity"

# System Configuration
system:
  num_workers: 4
  pin_memory: true
  prefetch_factor: 2
  dataloader_drop_last: true
  
  # Memory management
  empty_cache_steps: 1000  # Clear CUDA cache every N steps
  log_memory_usage: true
  
  # Logging
  log_level: "INFO"
  log_file: "training.log"
  use_wandb: false  # Weights & Biases integration
  wandb_project: "modernarabert"
  wandb_run_name: null  # Auto-generated if null

# Distributed Training (if using multiple GPUs)
distributed:
  enabled: false
  backend: "nccl"
  world_size: 1
  local_rank: -1

